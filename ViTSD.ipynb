{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708402476590,"user":{"displayName":"Thanh Nguyễn","userId":"18390529220867731119"},"user_tz":360},"id":"o6DZVAbiC_Vt","outputId":"27c346a0-9ab1-4704-b091-fbd58b53910e"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torchvision.datasets import DatasetFolder\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, auc\n","from sklearn.preprocessing import label_binarize\n","\n","np.random.seed(0)\n","torch.manual_seed(0)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":660,"status":"ok","timestamp":1708404950163,"user":{"displayName":"Thanh Nguyễn","userId":"18390529220867731119"},"user_tz":360},"id":"ghA48v6pRqZQ","outputId":"d62d035b-3e23-413e-d432-edfc392b5a44"},"outputs":[],"source":["# Define the transformations\n","transform = transforms.Compose([\n","    # transforms.Resize((224, 224)),\n","    # transforms.RandomHorizontalFlip(),\n","    # transforms.RandomRotation(degrees=15),\n","    # transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n","    transforms.Lambda(lambda x: x.to(torch.float32))\n","])\n","\n","root = '../dataset'\n","\n","# Initialize DatasetFolder for train and validation datasets\n","train_dataset = DatasetFolder(\n","    root=os.path.join(root, 'train'),\n","    loader=lambda x: torch.from_numpy(np.load(x)),\n","    extensions='npy',\n","    transform=transform\n",")\n","test_dataset = DatasetFolder(\n","    root=os.path.join(root, 'val'),\n","    loader=lambda x: torch.from_numpy(np.load(x)),\n","    extensions='npy',\n","    transform=transform\n",")\n","\n","# Split the train dataset to get the validation dataset\n","train_dataset, val_dataset = random_split(train_dataset, [int(len(train_dataset) * 0.9), int(len(train_dataset) * 0.1)])\n","\n","# Set the batch size\n","batch_size = 60\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Print some information about the data\n","print(f'Train dataset size: {len(train_dataset)}')\n","print(f'Val dataset size: {len(val_dataset)}')\n","print(f'Test dataset size: {len(test_dataset)}')\n","print(f'Image shape: {train_dataset[0][0].shape}')\n","print(f'Classes: {train_dataset.dataset.classes}')\n","print(f'Image tensor type: {train_dataset[0][0].dtype}')\n","print(f'Batches: {len(train_loader)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":806},"executionInfo":{"elapsed":46665,"status":"ok","timestamp":1708405057815,"user":{"displayName":"Thanh Nguyễn","userId":"18390529220867731119"},"user_tz":360},"id":"NTA8h9OFti1Y","outputId":"a1ae069a-b90b-44f8-d3e3-ab1c9c5659d9"},"outputs":[],"source":["dataiter = iter(train_loader)\n","images, labels = dataiter.__next__()\n","\n","# Visualize some images in the train dataset\n","fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n","\n","for i, ax in enumerate(axes.flat):\n","    if i < 16:\n","        image, label = images[i], labels[i]\n","\n","        img = image.numpy().transpose((1, 2, 0))\n","        img = np.clip(img, 0, 1)\n","\n","        ax.imshow(img)\n","        ax.set_title(train_dataset.dataset.classes[label])\n","        ax.axis('off')\n","    else:\n","        break\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(model, criterion, optimizer, scheduler=None, num_epochs=20, warmup_epochs=None, save_path='../best_param_ViT.pt'):\n","    history = {\n","        'epoch': [],\n","        'train_loss': [],\n","        'train_acc': [],\n","        'val_loss': [],\n","        'val_acc': []\n","    } # Initialize a dictionary to store epoch-wise results\n","    best_val_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        # Warm-up period\n","        if warmup_epochs is not None:\n","            if epoch < warmup_epochs:\n","                lr = 1e-13 * (10 ** epoch) \n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] = lr\n","\n","        # Training phase\n","        model.train()\n","        train_loss = 0.0\n","        train_corrects = 0\n","\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            \n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item() * inputs.size(0)\n","            _, preds = torch.max(F.softmax(outputs, dim=1), 1)\n","            train_corrects += torch.sum(preds == labels.data).item()\n","\n","        train_loss /= len(train_loader.dataset)\n","        train_acc = train_corrects / len(train_loader.dataset)\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0.0\n","        val_corrects = 0\n","\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * inputs.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                val_corrects += torch.sum(preds == labels.data).item()\n","\n","            val_loss /= len(val_loader.dataset)\n","            val_acc = val_corrects / len(val_loader.dataset)\n","\n","        # Append epoch results to history\n","        history['epoch'].append(epoch)\n","        history['train_loss'].append(train_loss)\n","        history['train_acc'].append(train_acc)\n","        history['val_loss'].append(val_loss)\n","        history['val_acc'].append(val_acc)\n","\n","        # Print results\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}] w/ LR = {optimizer.param_groups[0]['lr']}\")\n","        print(f\"\\tTrain Loss: {train_loss:.4f}\\tTrain Acc: {train_acc:.4f}\")\n","        print(f\"\\tVal Loss: {val_loss:.4f}\\tVal Acc: {val_acc:.4f}\")\n","\n","        # Step the scheduler\n","        if scheduler is not None:\n","            scheduler.step(val_loss)\n","\n","        # Save the parameters with the best validation accuracy\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), save_path)\n","\n","        model.load_state_dict(torch.load(save_path))\n","\n","    return history, model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qY55fM2sqkRq"},"outputs":[],"source":["def evaluate_model(model, criterion, model_name):\n","    model.eval()\n","    y_pred_probs = []\n","    y_true = []\n","    total_loss = 0.0\n","    total_corrects = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            total_loss += loss.item() * inputs.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            total_corrects += torch.sum(preds == labels).item()\n","\n","            y_pred_probs.extend(outputs.cpu().numpy())\n","            y_true.extend(labels.cpu().numpy())\n","\n","    test_loss = total_loss / len(test_loader.dataset)\n","    test_acc = total_corrects / len(test_loader.dataset)\n","\n","    # Binarize the labels for ROC AUC\n","    y_true_binary = label_binarize(y_true, classes=[0, 1, 2])\n","\n","    # Compute ROC AUC\n","    roc_auc = roc_auc_score(y_true_binary, y_pred_probs, multi_class='ovr')\n","    \n","    print(f'Test Loss: {test_loss:.4f}')\n","    print(f'Accuracy: {test_acc:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    classes = ['no', 'sphere', 'vort']\n","\n","    # Plot confusion matrix\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    cm = confusion_matrix(y_true, np.argmax(y_pred_probs, axis=1))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='icefire', xticklabels=classes, yticklabels=classes)\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","\n","    # Plot ROC curve for each class\n","    plt.subplot(1, 2, 2)\n","    fpr, tpr, _ = roc_curve(np.array(y_true_binary).ravel(), np.array(y_pred_probs).ravel())\n","    plt.plot(fpr, tpr, color='orange', label=f'{model_name}, {roc_auc:.2f}')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('ROC Curve')\n","    plt.legend(loc='lower right')\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def img_patch(images, num_patches):\n","    n, c, h, w = images.shape\n","    assert h == w, \"Patch method is implemented for square images only\"\n","\n","    patches = torch.zeros(n, num_patches ** 2, h * w * c // num_patches ** 2)\n","    patch_size = h // num_patches\n","\n","    for i, image in enumerate(images):\n","        for j in range(num_patches):\n","            for k in range(num_patches):\n","                patch = image[:, j * patch_size: (j + 1) * patch_size, k * patch_size: (k + 1) * patch_size]\n","                patches[i, j * num_patches + k] = patch.flatten()\n","\n","    return patches\n","\n","def get_positional_embeddings(seq_length, dim):\n","    result = torch.ones(seq_length, dim)\n","\n","    for i in range(seq_length):\n","        for j in range(dim):\n","            result[i][j] = np.sin(i / (10000 ** (j / dim))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / dim)))\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MultiheadSelfAttention(nn.Module):\n","    def __init__(self, dim, num_heads=4):\n","        super(MultiheadSelfAttention, self).__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","\n","        assert dim % num_heads == 0, f\"Can't divide dimension {dim} into {num_heads} heads\"\n","\n","        dim_head = int(dim / num_heads)\n","        self.q_mappings = nn.ModuleList([nn.Linear(dim_head, dim_head) for _ in range(self.num_heads)])\n","        self.k_mappings = nn.ModuleList([nn.Linear(dim_head, dim_head) for _ in range(self.num_heads)])\n","        self.v_mappings = nn.ModuleList([nn.Linear(dim_head, dim_head) for _ in range(self.num_heads)])\n","        self.dim_head = dim_head\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, sequences):\n","        # sequences has shape (N, seq_length, token_dim)\n","        # We go into shape    (N, seq_length, num_heads, token_dim / num_heads)\n","        # And come back to    (N, seq_length, item_dim) through concatenation\n","        result = []\n","\n","        for sequence in sequences:\n","            seq_result = []\n","            \n","            for head in range(self.num_heads):\n","                q_mapping = self.q_mappings[head]\n","                k_mapping = self.k_mappings[head]\n","                v_mapping = self.v_mappings[head]\n","\n","                seq = sequence[:, head * self.dim_head: (head + 1) * self.dim_head]\n","                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n","\n","                attention = self.softmax(q @ k.T / (self.dim_head ** 0.5))\n","                seq_result.append(attention @ v)\n","\n","            result.append(torch.hstack(seq_result))\n","\n","        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EncoderBlock(nn.Module):\n","    def __init__(self, hidden_dim, num_heads, mlp_ratio=4):\n","        super(EncoderBlock, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_heads = num_heads\n","\n","        self.norm1 = nn.LayerNorm(hidden_dim)\n","        self.mhsa = MultiheadSelfAttention(hidden_dim, num_heads)\n","        self.norm2 = nn.LayerNorm(hidden_dim)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_dim, mlp_ratio * hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(mlp_ratio * hidden_dim, hidden_dim)\n","        )\n","\n","    def forward(self, x):\n","        output = x + self.mhsa(self.norm1(x))\n","        output = output + self.mlp(self.norm2(output))\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ViTSD(nn.Module):\n","    def __init__(self, chw=(1, 150, 150), num_patches=15, num_blocks=8, hidden_dim=32, num_heads=4, out_dim=3):\n","        super(ViTSD, self).__init__()\n","        self.chw = chw # (C, H, W)\n","        self.num_patches = num_patches\n","        self.num_blocks = num_blocks\n","        self.hidden_dim = hidden_dim\n","        self.num_heads = num_heads\n","\n","        assert chw[1] % num_patches == 0, \"Input shape is not divisible by number of patches\"\n","        assert chw[2] % num_patches == 0, \"Input shape is not divisible by number of patches\"\n","        self.patch_size = (chw[1] / num_patches, chw[2] / num_patches)\n","\n","        # Linear mapper\n","        self.input_dim = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n","        self.linear_mapper = nn.Linear(self.input_dim, self.hidden_dim)\n","\n","        # Learnable classification token\n","        self.class_token = nn.Parameter(torch.rand(1, self.hidden_dim))\n","\n","        # Positional embedding\n","        self.register_buffer('positional_embeddings', get_positional_embeddings(num_patches ** 2 + 1, hidden_dim), persistent=False)\n","\n","        # Encoder blocks\n","        self.blocks = nn.ModuleList([EncoderBlock(hidden_dim, num_heads) for _ in range(num_blocks)])\n","\n","        # Classification MLP\n","        self.mlp = nn.Sequential(\n","            nn.Linear(self.hidden_dim * 226, out_dim), # Fix the constant manually\n","            nn.Softmax(dim=-1)\n","        )\n","\n","    def forward(self, images):\n","        # Divide images into patches\n","        n, c, h, w = images.shape\n","        patches = img_patch(images, self.num_patches).to(self.positional_embeddings.device)\n","\n","        # Run linear layer tokenization\n","        tokens = self.linear_mapper(patches)\n","\n","        # Add the classification token to tokens\n","        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n","\n","        # Add positional embedding\n","        output = tokens + self.positional_embeddings.repeat(n, 1, 1)\n","\n","        # Transformer blocks\n","        for block in self.blocks:\n","            output = block(output)\n","\n","        # Get the classification token only\n","        output = output.view(output.shape[0], -1)\n","        \n","        return self.mlp(output) # Map to output dimension, output category distribution"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["ViTSD(\n","  (linear_mapper): Linear(in_features=100, out_features=64, bias=True)\n","  (blocks): ModuleList(\n","    (0-7): 8 x EncoderBlock(\n","      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","      (mhsa): MultiheadSelfAttention(\n","        (q_mappings): ModuleList(\n","          (0-3): 4 x Linear(in_features=16, out_features=16, bias=True)\n","        )\n","        (k_mappings): ModuleList(\n","          (0-3): 4 x Linear(in_features=16, out_features=16, bias=True)\n","        )\n","        (v_mappings): ModuleList(\n","          (0-3): 4 x Linear(in_features=16, out_features=16, bias=True)\n","        )\n","        (softmax): Softmax(dim=-1)\n","      )\n","      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","      (mlp): Sequential(\n","        (0): Linear(in_features=64, out_features=256, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=256, out_features=64, bias=True)\n","      )\n","    )\n","  )\n","  (mlp): Sequential(\n","    (0): Linear(in_features=14464, out_features=3, bias=True)\n","    (1): Softmax(dim=-1)\n","  )\n",")"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize the model\n","model = ViTSD(num_blocks=8, hidden_dim=64, num_heads=4).to(device)\n","\n","# Learning parameters\n","optimizer = optim.AdamW(model.parameters(), lr=1e-6)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5)\n","criterion = nn.CrossEntropyLoss()\n","\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L9D85FshfEO1","outputId":"49244e26-c3f4-4710-a249-90491eb27e02"},"outputs":[],"source":["history, model = train_model(model, criterion, optimizer, scheduler, num_epochs=50)\n","print('Finished')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYExMGzQe75M","outputId":"34262628-db40-422e-e8a9-3df3acf7b761"},"outputs":[],"source":["model.load_state_dict(torch.load('../best_param_ViT.pt'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBYd1GKFELq9","outputId":"4f923f0d-d2ea-49a7-94f6-bba2d15cc492"},"outputs":[],"source":["evaluate_model(model, criterion, 'ViTSD')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
