{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708402476590,"user":{"displayName":"Thanh Nguyễn","userId":"18390529220867731119"},"user_tz":360},"id":"o6DZVAbiC_Vt","outputId":"27c346a0-9ab1-4704-b091-fbd58b53910e"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torchvision.datasets import DatasetFolder\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, auc\n","from sklearn.preprocessing import label_binarize\n","\n","np.random.seed(0)\n","torch.manual_seed(0)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":660,"status":"ok","timestamp":1708404950163,"user":{"displayName":"Thanh Nguyễn","userId":"18390529220867731119"},"user_tz":360},"id":"ghA48v6pRqZQ","outputId":"d62d035b-3e23-413e-d432-edfc392b5a44"},"outputs":[],"source":["# Define the transformations\n","transform = transforms.Compose([\n","    # transforms.Resize((224, 224)),\n","    # transforms.RandomHorizontalFlip(),\n","    # transforms.RandomRotation(degrees=15),\n","    # transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n","    transforms.Lambda(lambda x: x.to(torch.float32))\n","])\n","\n","root = '../dataset'\n","\n","# Initialize DatasetFolder for train and validation datasets\n","train_dataset = DatasetFolder(\n","    root=os.path.join(root, 'train'),\n","    loader=lambda x: torch.from_numpy(np.load(x)),\n","    extensions='npy',\n","    transform=transform\n",")\n","test_dataset = DatasetFolder(\n","    root=os.path.join(root, 'val'),\n","    loader=lambda x: torch.from_numpy(np.load(x)),\n","    extensions='npy',\n","    transform=transform\n",")\n","\n","# Split the train dataset to get the validation dataset\n","train_dataset, val_dataset = random_split(train_dataset, [int(len(train_dataset) * 0.9), int(len(train_dataset) * 0.1)])\n","\n","# Set the batch size\n","batch_size = 60\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Print some information about the data\n","print(f'Train dataset size: {len(train_dataset)}')\n","print(f'Val dataset size: {len(val_dataset)}')\n","print(f'Test dataset size: {len(test_dataset)}')\n","print(f'Image shape: {train_dataset[0][0].shape}')\n","print(f'Classes: {train_dataset.dataset.classes}')\n","print(f'Image tensor type: {train_dataset[0][0].dtype}')\n","print(f'Batches: {len(train_loader)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":806},"executionInfo":{"elapsed":46665,"status":"ok","timestamp":1708405057815,"user":{"displayName":"Thanh Nguyễn","userId":"18390529220867731119"},"user_tz":360},"id":"NTA8h9OFti1Y","outputId":"a1ae069a-b90b-44f8-d3e3-ab1c9c5659d9"},"outputs":[],"source":["dataiter = iter(train_loader)\n","images, labels = dataiter.__next__()\n","\n","# Visualize some images in the train dataset\n","fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n","\n","for i, ax in enumerate(axes.flat):\n","    if i < 16:\n","        image, label = images[i], labels[i]\n","\n","        img = image.numpy().transpose((1, 2, 0))\n","        img = np.clip(img, 0, 1)\n","\n","        ax.imshow(img)\n","        ax.set_title(train_dataset.dataset.classes[label])\n","        ax.axis('off')\n","    else:\n","        break\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(model, criterion, optimizer, scheduler=None, num_epochs=20, warmup_epochs=None, save_path='../best_param_ViT.pt'):\n","    history = {\n","        'epoch': [],\n","        'train_loss': [],\n","        'train_acc': [],\n","        'val_loss': [],\n","        'val_acc': []\n","    } # Initialize a dictionary to store epoch-wise results\n","    best_val_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        # Warm-up period\n","        if warmup_epochs is not None:\n","            if epoch < warmup_epochs:\n","                lr = 1e-13 * (10 ** epoch) \n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] = lr\n","\n","        # Training phase\n","        model.train()\n","        train_loss = 0.0\n","        train_corrects = 0\n","\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            \n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item() * inputs.size(0)\n","            _, preds = torch.max(F.softmax(outputs, dim=1), 1)\n","            train_corrects += torch.sum(preds == labels.data).item()\n","\n","        train_loss /= len(train_loader.dataset)\n","        train_acc = train_corrects / len(train_loader.dataset)\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0.0\n","        val_corrects = 0\n","\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * inputs.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                val_corrects += torch.sum(preds == labels.data).item()\n","\n","            val_loss /= len(val_loader.dataset)\n","            val_acc = val_corrects / len(val_loader.dataset)\n","\n","        # Append epoch results to history\n","        history['epoch'].append(epoch)\n","        history['train_loss'].append(train_loss)\n","        history['train_acc'].append(train_acc)\n","        history['val_loss'].append(val_loss)\n","        history['val_acc'].append(val_acc)\n","\n","        # Print results\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}] w/ LR = {optimizer.param_groups[0]['lr']}\")\n","        print(f\"\\tTrain Loss: {train_loss:.4f}\\tTrain Acc: {train_acc:.4f}\")\n","        print(f\"\\tVal Loss: {val_loss:.4f}\\tVal Acc: {val_acc:.4f}\")\n","\n","        # Step the scheduler\n","        if scheduler is not None:\n","            scheduler.step(val_loss)\n","\n","        # Save the parameters with the best validation accuracy\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            torch.save(model.state_dict(), save_path)\n","\n","        model.load_state_dict(torch.load(save_path))\n","\n","    return history, model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qY55fM2sqkRq"},"outputs":[],"source":["def evaluate_model(model, criterion, model_name):\n","    model.eval()\n","    y_pred_probs = []\n","    y_true = []\n","    total_loss = 0.0\n","    total_corrects = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            total_loss += loss.item() * inputs.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            total_corrects += torch.sum(preds == labels).item()\n","\n","            y_pred_probs.extend(outputs.cpu().numpy())\n","            y_true.extend(labels.cpu().numpy())\n","\n","    test_loss = total_loss / len(test_loader.dataset)\n","    test_acc = total_corrects / len(test_loader.dataset)\n","\n","    # Binarize the labels for ROC AUC\n","    y_true_binary = label_binarize(y_true, classes=[0, 1, 2])\n","\n","    # Compute ROC AUC\n","    roc_auc = roc_auc_score(y_true_binary, y_pred_probs, multi_class='ovr')\n","    \n","    print(f'Test Loss: {test_loss:.4f}')\n","    print(f'Accuracy: {test_acc:.4f}')\n","    print(f'ROC AUC: {roc_auc:.4f}')\n","    classes = ['no', 'sphere', 'vort']\n","\n","    # Plot confusion matrix\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    cm = confusion_matrix(y_true, np.argmax(y_pred_probs, axis=1))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='icefire', xticklabels=classes, yticklabels=classes)\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","\n","    # Plot ROC curve for each class\n","    plt.subplot(1, 2, 2)\n","    fpr, tpr, _ = roc_curve(np.array(y_true_binary).ravel(), np.array(y_pred_probs).ravel())\n","    plt.plot(fpr, tpr, color='orange', label=f'{model_name}, {roc_auc:.2f}')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('ROC Curve')\n","    plt.legend(loc='lower right')\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Hyperparameters\n","LEARNING_RATE = 1e-4\n","NUM_CLASSES = 3\n","PATCH_SIZE = 15\n","IMG_SIZE = 150\n","IN_CHANNELS = 1\n","NUM_HEADS = 8\n","DROP_OUT = 0.001\n","HIDDEN_DIM = 256\n","ADAM_WEIGHT_DECAY = 0\n","ADAM_BETAS = (0.9, 0.999)\n","ACTIVATION = 'gelu'\n","NUM_ENCODERS = 4\n","EMBED_DIM = (PATCH_SIZE ** 2) * IN_CHANNELS\n","NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Patch embedding\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):\n","        super().__init__()\n","        self.patcher = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=in_channels,out_channels=embed_dim, kernel_size=patch_size, stride=patch_size),\n","            nn.Flatten(2)\n","        )\n","        self.cls_token = nn.Parameter(torch.randn(1, in_channels, embed_dim), requires_grad=True)\n","        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim), requires_grad=True)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, x):\n","        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n","\n","        x = self.patcher(x).permute(0, 2, 1)\n","        x = torch.cat([cls_token, x], dim=1)\n","        x = self.position_embeddings + x\n","        x = self.dropout(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ViT class\n","class ViT(nn.Module):\n","    def __init__(\n","        self, num_patches, img_size, num_classes,\n","        patch_size, embed_dim, num_encoders, num_heads,\n","        hidden_dim, dropout, activation, in_channels\n","    ):\n","        super.__init__()\n","        self.embeddings_block = PatchEmbedding(embed_dim, patch_size, num_patches, dropout, in_channels)\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=embed_dim,\n","            nhead=num_heads,\n","            dropout=dropout,\n","            activation=activation,\n","            batch_first=True,\n","            norm_first=True\n","        )\n","        self.encoder_blocks = nn.TransformerEncoder(encoder_layer, num_layers=num_encoders)\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(normalized_shape=embed_dim),\n","            nn.Linear(in_features=embed_dim, out_features=num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.embeddings_block(x)\n","        x = self.encoder_blocks(x)\n","        x = self.mlp_head(x[:, 0, :])\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = ViT(\n","    NUM_PATCHES, IMG_SIZE, NUM_CLASSES, PATCH_SIZE,\n","    EMBED_DIM, NUM_ENCODERS, NUM_HEADS, HIDDEN_DIM,\n","    DROP_OUT, ACTIVATION, IN_CHANNELS\n",").to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L9D85FshfEO1","outputId":"49244e26-c3f4-4710-a249-90491eb27e02"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr = LEARNING_RATE)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n","\n","history, model = train_model(model, criterion, optimizer, scheduler, num_epochs=50)\n","print('Finished')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYExMGzQe75M","outputId":"34262628-db40-422e-e8a9-3df3acf7b761"},"outputs":[],"source":["model.load_state_dict(torch.load('../best_param_ViT.pt'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBYd1GKFELq9","outputId":"4f923f0d-d2ea-49a7-94f6-bba2d15cc492"},"outputs":[],"source":["evaluate_model(model, criterion, 'ViTSD')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
